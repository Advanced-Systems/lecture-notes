\subsection{Diagonalization}\label{subsubsec-diagonalization}

\begin{thm}\label{thm-diagonalizable-linear-map}
	Let $T:\mathcal{V}\to\mathcal{V}$ be a linear operator. Then $T$ has many matrix
	representations. Given a basis $\mathcal{B}$, we can construct the representation
	matrix of $T$ with respect to this basis as $[T]_\mathcal{B}$. Suppose there exists
	a basis $\mathcal{B}=\{v_1,\dots,v_n\}$ s.t. the following properties hold:
	\begin{flushleft}
		Suppose $T(v_i)=\lambda_i v_i$ for all $i\in\{1,\dots,n\}$. Then the matrix
		representation of this particular linear transformation with respect to
		$\mathcal{B}$ is
		\begin{equation}
			[T]_\mathcal{B}=\begin{pmatrix}
				\lambda_1 & 0         & 0         & \cdots & 0         \\
				0         & \lambda_1 & 0         & \cdots & 0         \\
				0         & 0         & \lambda_1 & \cdots & 0         \\
				\vdots    & \vdots    & \vdots    & \ddots & \vdots    \\
				0         & 0         & 0         & \cdots & \lambda_1
			\end{pmatrix}=\fdiag(\lambda_1,\dots,\lambda_n)
		\end{equation}
	\end{flushleft}
	We say $T$ is diagonalizable if such a basis $\mathcal{B}$ exists.
\end{thm}

\begin{rem}\label{rem-linear-map-diagonalizable}
	We know that given a linear map $T$, we write its matrix representation as
	$A=[T]_\mathcal{F}$ for a basis $\mathcal{F}$. Conversely, we can construct
	a linear map from a matrix $A$ by defining $T(v)\defines Av$. Hence, we say
	a matrix is diagonalizable if $A \sim B$ where $B$ is a diagonal matrix,
	\textit{i.e.} $\fdiag(\lambda_1,\dots,\lambda_n)=P^{-1}AP$.
\end{rem}

\begin{rem}
	\pref{Remark}{rem-linear-map-diagonalizable} gives rise to the following two questions:
	\begin{itemize}
		\item How do we know if a linear map or matrix is diagonalizable?
		\item And if it is, how do we find such a basis $\mathcal{B}$ from which
		      we can obtain this diagonal matrix?
	\end{itemize}
\end{rem}

\subsubsection{Eigenvectors and Eigenvalues}\label{subsubsec-eigenvectors-and-eigenvalues}

\begin{definition}\label{def-eigenvector-eigenvalue}
	A non-zero vector $v\in\mathcal{V}$ s.t. $T(v)=\lambda v$ is called an eigenvector
	of $T$\footnote{Note that the zero vector always satisfies this definition.}.
	Furthermore, $\lambda$ is called an eigenvalue of $T$ corresponding to $v$.
\end{definition}

\begin{definition}\label{def-diagonalizable-linear-map}
	A linear map $T$ is diagonalizable \textit{iff} it has a basis of eigenvectors.
	Moreover, the diagonal matrix has the corresponding eigenvalues on its main
	diagonal as entries, \textit{i.e.} $\fdiag(\lambda_1,\dots,\lambda_n)$.
\end{definition}

\begin{rem}
	Before we head into the next big theorem, by way of introduction we will work through
	\pref{example}{exm-eigenvectors-eigenvalues} in detail to take off the burden of abstraction.
\end{rem}

\begin{exm}\label{exm-eigenvectors-eigenvalues}
	Let $A=\inlinematrix{1&2\\3&2}$. Let $T(v)=Av$,
	\textit{i.e.}
	\begin{equation*}
		T\begin{pmatrix}
			x \\y
		\end{pmatrix}=\begin{pmatrix}
			1 & 2 \\
			3 & 2
		\end{pmatrix}\begin{pmatrix}
			x \\ y
		\end{pmatrix}=\begin{pmatrix}
			x + 2y \\
			3x + 2y
		\end{pmatrix}
	\end{equation*}
	We are looking for a non-zero vector $\inlinematrix{x\\y}$
	and a scalar $\lambda$ s.t. $T(v)=\lambda v$, \textit{i.e.}\footnote{Remark:
		$\lambda\left(
			\begin{smallmatrix}
				x\\y
			\end{smallmatrix}
			\right)=\left(
			\begin{smallmatrix}
				\lambda & 0 \\
				0 & \lambda
			\end{smallmatrix}
			\right)\left(
			\begin{smallmatrix}
				x \\ y
			\end{smallmatrix}
			\right)$}
	\begin{align*}
		 & \begin{pmatrix}
			1 & 2 \\
			3 & 2
		\end{pmatrix}\begin{pmatrix}
			x \\ y
		\end{pmatrix}=\lambda\begin{pmatrix}
			x \\ y
		\end{pmatrix}                            \\
		\Leftrightarrow
		 & \begin{pmatrix}
			1 & 2 \\
			3 & 2
		\end{pmatrix}\begin{pmatrix}
			x \\ y
		\end{pmatrix}-\lambda\begin{pmatrix}
			x \\ y
		\end{pmatrix}=\begin{pmatrix}
			0 \\ 0
		\end{pmatrix} \\
		\Leftrightarrow
		 & \Bigg(\begin{pmatrix}
			1 & 2 \\
			3 & 2
		\end{pmatrix}-\lambda\cdot I_2\Bigg)\begin{pmatrix}
			x \\ y
		\end{pmatrix}=\begin{pmatrix}
			0 \\ 0
		\end{pmatrix}      \\
		\Leftrightarrow
		 & \begin{pmatrix}
			1-\lambda & 2         \\
			3         & 2-\lambda
		\end{pmatrix}\begin{pmatrix}
			x \\ y
		\end{pmatrix}=\begin{pmatrix}
			0 \\ 0
		\end{pmatrix}
	\end{align*}
	So, in other words we are looking for an eigenvector
	$v=\inlinematrix{x\\y}\in\fker{A-\lambda\cdot I}$
	that solves the homogeneous system
	\begin{equation}\label{eq-homogeneous-system-eigenvectors}
		(A-\lambda\cdot I_2)v=0
	\end{equation}
	Here we are only interested in the non-trivial solutions of this system (since
	by \pref{theorem}{thm-homogeneous-inf-solutions} a homogeneous systems has either
	infinitely many solutions or only the trivial solution). We will denote
	$M\defines A-\lambda\cdot I$. There is a unique solution \textit{iff} the rank
	is full, \textit{i.e.} $\frank(M)=2 \Leftrightarrow \det(M)\neq0$ by
	\pref{theorem}{thm-square-matrix-properties}. The complement of this theorem
	states that there are infinitely many solutions \textit{iff}
	\begin{align*}
		 & \det(M)=0                    \\
		\Leftrightarrow
		 & \begin{vmatrix}
			1-\lambda & 2         \\
			3         & 2-\lambda
		\end{vmatrix}=0 \\
		\Leftrightarrow
		 & (1-\lambda)(2-\lambda)-6=0   \\
		\Leftrightarrow
		 & \lambda^2-3\lambda-4=0
	\end{align*}
	This polynomial is known as the characteristic polynomial of $A$, and its roots
	are the eigenvalues.
	\begin{equation*}
		p(\lambda)=(\lambda+1)(\lambda-4) \implies \lambda_1=-1,\lambda_2=4
	\end{equation*}
	To find the corresponding eigenvectors, we plug in $\lambda_1$ and $\lambda_2$
	into the system given by \pref{equation}{eq-homogeneous-system-eigenvectors}:
	\begin{align*}
		\boxed{\lambda_1=-1}: & \begin{pmatrix}
			2 & 2 \\
			3 & 3
		\end{pmatrix}\begin{pmatrix}
			x \\ y
		\end{pmatrix}=\begin{pmatrix}
			0 \\ 0
		\end{pmatrix}\implies x=-y            \\
		\boxed{\lambda_2=4}:  & \begin{pmatrix}
			-3 & 2  \\
			3  & -2
		\end{pmatrix}\begin{pmatrix}
			x \\ y
		\end{pmatrix}=\begin{pmatrix}
			0 \\ 0
		\end{pmatrix}\implies y=\tfrac{3}{2}x
	\end{align*}
	So a general for $\lambda_1$ is of the form
	$\inlinematrix{x\\-x}$, \textit{i.e.}
	\begin{equation*}
		x=1:\begin{pmatrix}
			1 & 2 \\
			3 & 2
		\end{pmatrix}\begin{pmatrix}
			1 \\ -1
		\end{pmatrix}=-1\cdot\begin{pmatrix}
			1 \\ -1
		\end{pmatrix}
	\end{equation*}
	Similarly, verify that for $\lambda_2$ we have a general solution of the form
	$\inlinematrix{x\\\tfrac{3}{2}x}$, \textit{i.e.}
	\begin{equation*}
		x=1:\begin{pmatrix}
			1 & 2 \\
			3 & 2
		\end{pmatrix}\begin{pmatrix}
			1 \\ \tfrac{3}{2}
		\end{pmatrix}=4\cdot\begin{pmatrix}
			1 \\ \tfrac{3}{2}
		\end{pmatrix}
	\end{equation*}
\end{exm}

\begin{rem}
	Any two vectors of the form $\inlinematrix{x\\-x}$
	are linearly dependent, likewise any two vectors of the form $\inlinematrix{x\\\tfrac{3}{2}x}$
	are also linearly dependent. But $\inlinematrix{x\\-x}$ and
	$\inlinematrix{x\\\tfrac{3}{2}x}$ are linearly independent.
	Define
	\begin{equation*}
		\mathcal{B}=\left\{\begin{pmatrix}
			1 \\ -1
		\end{pmatrix},
		\begin{pmatrix}[r]
			1 \\ \frac{3}{2}
		\end{pmatrix}\right\}
	\end{equation*}
	as a basis of eigenvectors. Therefore,
	\begin{align*}
		T\begin{pmatrix}1\\-1\end{pmatrix} & =-1\cdot\begin{pmatrix}1\\-1\end{pmatrix} \\
		T\begin{pmatrix}1\\\frac{3}{2}\end{pmatrix} & =4\cdot\begin{pmatrix}1\\\frac{3}{2}\end{pmatrix}
	\end{align*}
	Hence, the matrix representation for this system is the diagonal matrix of the eigenvalues:
	\begin{equation*}
		[T]_\mathcal{B}=\begin{pmatrix}
			-1 & 0 \\
			0  & 4
		\end{pmatrix}=\fdiag(-1,4)
	\end{equation*}
	Note that $A \sim \fdiag(-1,4)$, so $\fdiag(-1,4)=P^{-1}AP$. The change of basis
	matrix $P$ equals $P=\inlinematrix{1&1\\-1&\frac{3}{2}}$,
	\textit{i.e.} the columns of $P$ are the eigenvectors.
\end{rem}

\begin{definition}\label{def-determinant-kernel-alt}
	Suppose $T:\mathcal{V}\to\mathcal{V}$ is a linear operator. Then we define
	\footnote{$\abs{T}$ does not dependent on the choice of $\mathcal{F}$, since
		any two matrix representation of $T$ are similar be \pref{definition}{def-similar-matrices}})
	$\abs{T}=\abs{[T]_\mathcal{F}}$. Likewise we will define $\fker{A}$ for all
	$v$'s such that $Av=0$.
\end{definition}

\begin{thm}\label{thm-eigenvalue-eigenvector}
	Let $T:\mathcal{V}\to\mathcal{V}$ be a linear operator. Then
	\begin{enumerate}
		\item The eigenvalues of $T$ are the roots of the so-called characteristic polynomial $\det(T-\lambda I)$
		\item If $\lambda$ is an eigenvalue, then its corresponding eigenvectors are the non-zero vectors in $\fker{T-\lambda I}$
	\end{enumerate}
\end{thm}

\begin{rem}
	Before we go on to prove \pref{theorem}{thm-eigenvalue-eigenvector} let us
	first consider a couple of remarks that will help us understand what we need
	to show.
	\begin{enumerate}
		\item An analogous theorem can be written for a matrix $A$.
		\item The set of all eigenvectors corresponding to an eigenvalue $\lambda$
		      including the zero vector is a subspace of $\mathcal{V}$ (since it is a
		      kernel). This is also known as the eigenspace of $\mathcal{V}$ corresponding
		      to the map $T$ and the eigenvalue $\lambda$, and is denoted by $\mathcal{V}_\lambda$.
	\end{enumerate}
\end{rem}

\begin{proof}
	Of \pref{theorem}{thm-eigenvalue-eigenvector}.
	\begin{flushleft}
		\textbf{Statement 1}: Suppose there are eigenvectors corresponding to $\lambda$. Then,
		\begin{align*}
			 & \fker{T - \lambda I}\neq\{0\}                                                                               \\
			\Leftrightarrow
			 & T(v) - \lambda I\text{ is not injective}  &  & \text{\pref{theorem}{thm-kernel-image-subspace}}             \\
			\Leftrightarrow
			 & T(v) - \lambda I\text{ is not invertible} &  & \text{\pref{theorem}{thm-linear-operator-invertible}}        \\
			\Leftrightarrow
			 & \det(T(v) - \lambda I) = 0                &  & \text{\pref{theorem}{thm-square-matrix-properties-extended}}
		\end{align*}
	\end{flushleft}
	\begin{flushleft}
		\textbf{Statement 2}: Let $v\neq0$ be an eigenvector. Then,
		\begin{align*}
			 & T(v) = \lambda v         &  & \text{\pref{definition}{def-eigenvector-eigenvalue}} \\
			\Leftrightarrow
			 & T(v) - \lambda v = 0                                                               \\
			\Leftrightarrow
			 & T(v) - \lambda I(v) = 0                                                            \\
			\Leftrightarrow
			 & (T - \lambda I)(v) = 0   &  & \text{\pref{definition}{def-linear-maps-operations}} \\
			\Leftrightarrow
			 & v\in\fker{T - \lambda I} &  & \text{\pref{definition}{def-kernel-linear-map}}
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{thm}\label{thm-eigenvalue-eigenvector-linearly-independent}
	Eigenvectors corresponding to different eigenvalues are linearly independent.
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-eigenvalue-eigenvector-linearly-independent}.
	\begin{flushleft}
		Let $\lambda_1,\dots,\lambda_k$ be different eigenvalues, and let $v_1,\dots,v_k$
		be corresponding eigenvectors. We will show by induction that the set $v_1,\dots,v_k$
		is linearly independent.
	\end{flushleft}
	\begin{flushleft}
		\textbf{Base Clause}:
		\begin{align*}
			 & v_1\neq0                                       &  & \text{\pref{definition}{def-eigenvector-eigenvalue}} \\
			 & \implies\{v_1\}\text{ is linearly independent}
		\end{align*}
	\end{flushleft}
	\begin{flushleft}
		\textbf{Induction hypothesis}: Suppose that the set $v_1,\dots,v_{k-1}$ is linearly independent.
	\end{flushleft}
	\begin{flushleft}
		\textbf{Induction Step}: Consider $\mu_1v_1+\cdots+\mu_kv_k=0$. Then it follows that
		\begin{align}
			 & \lambda_1(\mu_1v_1+\cdots+\mu_kv_k)=0\nonumber \\
			 & \Leftrightarrow
			\mu_1\lambda_1v_1+\cdots+\mu_k\lambda_1v_k=0\label{eq-tmp:1}
		\end{align}
		On the other hand,
		\begin{equation*}
			T(0)=T(\mu_1v_1+\cdots+\mu_kv_k)=0
		\end{equation*}
		But this equals
		\begin{align}
			 & \mu_1T(v_1)+\cdots+\mu_kT(v_k)=0\nonumber                    \\
			\Leftrightarrow
			 & \mu_1\lambda_1v_1+\cdots+\mu_k\lambda_kv_k=0\label{eq-tmp:2}
		\end{align}
		Therefore it follows from \pref{equation}{eq-tmp:1}) and \pref{equation}{eq-tmp:2} that
		\begin{align*}
			 & \mu_1\lambda_1v_1+\cdots+\mu_k\lambda_1v_k=\mu_1\lambda_1v_1+\cdots+\mu_k\lambda_kv_k \\
			\Leftrightarrow
			 & 0+\mu_2(\lambda_2-\lambda_1)v_2+\cdots+\mu_k(\lambda_k-\lambda_1)v_k=0
		\end{align*}
		So, by the induction hypothesis,
		\begin{equation*}
			\mu_2(\lambda_2-\lambda_1)=\cdots=\mu_k(\lambda_k-\lambda_1)=0
		\end{equation*}
		Since $\lambda_i\neq\lambda_j$ for $i\neq j$ it automatically follows
		that $\mu_1=\cdots=\mu_k=0$. Hence, $\mu_1v_1=0 \implies \mu_1=0$.
	\end{flushleft}
\end{proof}

\begin{thm}\label{thm-change-of-base-matrix-eigenvalues}
	Let $A\in\mathcal{M}_n(\mathcal{F})$ be a diagonalizable matrix, and
	$B=\{v_1,\dots,v_n\}$ be a basis of eigenvectors with corresponding
	eigenvalues $\lambda_1,\dots,\lambda_n$. Then the matrix $P$ whose columns
	are the $v_i$'s satisfy
	\begin{equation*}
		P^{-1}AP=\begin{pmatrix}
			\lambda_1 & 0         & 0         & \cdots & 0         \\
			0         & \lambda_2 & 0         & \cdots & 0         \\
			0         & 0         & \lambda_3 & \cdots & 0         \\
			\vdots    & \vdots    & \vdots    & \ddots & \vdots    \\
			0         & 0         & 0         & \cdots & \lambda_n
		\end{pmatrix}=\fdiag(\lambda_1,\dots,\lambda_n)
	\end{equation*}
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-change-of-base-matrix-eigenvalues}.
	\begin{flushleft}
		We will show instead that
		\begin{equation*}
			AP=P\fdiag(\lambda_1,\dots,\lambda_n)
		\end{equation*}
	\end{flushleft}
	\begin{flushleft}
		\proofright: When we evaluate the right-hand side we get
		\begin{align*}
			AP & =A\begin{pmatrix}
				\vertbar & \vertbar &        & \vertbar \\
				v_1      & v_2      & \cdots & v_n      \\
				\vertbar & \vertbar &        & \vertbar
			\end{pmatrix} \\
			   & =\begin{pmatrix}
				\vertbar & \vertbar &        & \vertbar \\
				Av_1     & Av_2     & \cdots & Av_n     \\
				\vertbar & \vertbar &        & \vertbar
			\end{pmatrix}  \\
			   & =\begin{pmatrix}
				\vertbar      & \vertbar      &        & \vertbar      \\
				\lambda_1 v_1 & \lambda_2 v_2 & \cdots & \lambda_n v_n \\
				\vertbar      & \vertbar      &        & \vertbar
			\end{pmatrix}
		\end{align*}
	\end{flushleft}
	\begin{flushleft}
		\proofleft: When we evaluate the left-hand side we get
		\begin{align*}
			P\fdiag(\lambda_1,\dots,\lambda_n) & =
			\begin{pmatrix}
				v_{11} & v_{21} & \cdots & v_{n1} \\
				v_{12} & v_{22} & \cdots & v_{n2} \\
				\vdots & \vdots & \ddots & \vdots \\
				v_{1n} & v_{2n} & \cdots & v_{nn}
			\end{pmatrix}
			\begin{pmatrix}
				\lambda_1 & 0         & 0         & \cdots & 0         \\
				0         & \lambda_2 & 0         & \cdots & 0         \\
				0         & 0         & \lambda_3 & \cdots & 0         \\
				\vdots    & \vdots    & \vdots    & \ddots & \vdots    \\
				0         & 0         & 0         & \cdots & \lambda_n
			\end{pmatrix}                                       \\
			                                   & =\begin{pmatrix}
				\lambda_1 v_{11} & \lambda_2 v_{21} & \cdots & \lambda_n v_{n1} \\
				\lambda_1 v_{12} & \lambda_2 v_{22} & \cdots & \lambda_n v_{n2} \\
				\vdots           & \vdots           & \ddots & \vdots           \\
				\lambda_1 v_{1n} & \lambda_2 v_{2n} & \cdots & \lambda_n v_{nn}
			\end{pmatrix} \\
			                                   & =\begin{pmatrix}
				\vertbar      & \vertbar      &        & \vertbar      \\
				\lambda_1 v_1 & \lambda_2 v_2 & \cdots & \lambda_n v_n \\
				\vertbar      & \vertbar      &        & \vertbar
			\end{pmatrix}
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{exm}\label{exm-algebraic-geometrix-multiplicity}
	Let $A=
		\left(
		\begin{smallmatrix}
				1 & -3 & 3 \\
				3 & -5 & 3 \\
				6 & -6 & 4
			\end{smallmatrix}
		\right)$ where $T:\mathbb{R}^3\to\mathbb{R}^3,T(v)=Av$. Find the eigenvectors and
	their corresponding eigenvalues for this matrix.
	\begin{flushleft}
		\textbf{Answer}: What we first have to do is find the characteristic polynomial:
		\begin{align*}
			\det(A - \lambda I) & = \begin{vmatrix}
				1-\lambda & -3         & 3         \\
				3         & -5-\lambda & 3         \\
				6         & -6         & 4-\lambda
			\end{vmatrix}                                                  \\
			                    & = \begin{vmatrix}
				-2-\lambda & -3         & 3         \\
				-2-\lambda & -5-\lambda & 3         \\
				0          & -6         & 4-\lambda
			\end{vmatrix}              &  & \text{$C_1\leftarrow C_1+C_2$} \\
			                    & = (-2-\lambda) \begin{vmatrix}
				1 & -3         & 3         \\
				1 & -5-\lambda & 3         \\
				0 & -6         & 4-\lambda
			\end{vmatrix}                                     \\
			                    & = -(2+\lambda) \begin{vmatrix}
				1 & -3         & 3         \\
				0 & -2-\lambda & 0         \\
				0 & -6         & 4-\lambda
			\end{vmatrix} &  & \text{$R_2\leftarrow R_2-R_1$} \\
			                    & = -(2+\lambda) \begin{vmatrix}
				-2-\lambda & 0         \\
				-6         & 4-\lambda
			\end{vmatrix}                                     \\
			                    & = (2+\lambda)^2(4-\lambda)
		\end{align*}
		So, the eigenvalues are $\{-2,4\}$. Observe that $-2$ is a root of
		algebraic multiplicity $2$, since it occurs twice as a solution to the
		characteristic polynomial, whereas $-4$ has an algebraic multiplicity of $1$.
		\begin{align*}
			\boxed{\lambda=-2}:\quad A- \lambda I & = \begin{pmatrix}
				1-(-2) & -3      & 3      \\
				3      & -5-(-2) & 3      \\
				6      & -6      & 4-(-2)
			\end{pmatrix} \\
			                                      & = \begin{pmatrix}
				3 & -3 & 3 \\
				3 & -3 & 3 \\
				6 & -6 & 6
			\end{pmatrix}
		\end{align*}
		The eigenvectors corresponding to $\lambda=-2$ are in the kernel of
		\begin{equation*}
			\begin{pmatrix}
				3 & -3 & 3 \\
				3 & -3 & 3 \\
				6 & -6 & 6
			\end{pmatrix}\begin{pmatrix}
				x \\ y \\ z
			\end{pmatrix}=
			\begin{pmatrix}
				0 \\ 0 \\ 0
			\end{pmatrix}\implies z=y-x
		\end{equation*}
		Therefore, a general solution is of the form $\inlinematrix{x\\y\\y-x}$,
		\textit{i.e.} we have two degrees of freedom:
		\begin{equation*}
			v_1=\begin{pmatrix}
				1 \\ 0 \\ -1
			\end{pmatrix},v_2=\begin{pmatrix}
				0 \\ 1 \\ 1
			\end{pmatrix}
		\end{equation*}
		So, $\lambda=-2$ has a geometric multiplicity of $2$.
		\begin{align*}
			\boxed{\lambda=4}:\quad A- \lambda I & = \begin{pmatrix}
				1-(4) & -3     & 3     \\
				3     & -5-(4) & 3     \\
				6     & -6     & 4-(4)
			\end{pmatrix} \\
			                                     & = \begin{pmatrix}
				-3 & -3 & 3 \\
				3  & -9 & 3 \\
				6  & -6 & 0
			\end{pmatrix}
		\end{align*}
		Similarly, the eigenvectors corresponding to $\lambda=4$ are in the kernel of
		\begin{equation*}
			\begin{pmatrix}
				-3 & -3 & 3 \\
				3  & -9 & 3 \\
				6  & -6 & 0
			\end{pmatrix}\begin{pmatrix}
				x \\ y \\ z
			\end{pmatrix}=
			\begin{pmatrix}
				0 \\ 0 \\ 0
			\end{pmatrix}\implies\begin{pmatrix}
				1 & 0 & -\frac{1}{2} \\[4pt]
				0 & 1 & -\frac{1}{2} \\[4pt]
				0 & 0 & 0
			\end{pmatrix}\implies\begin{cases}
				x - \frac{z}{2} = 0 \\
				y - \frac{z}{2} = 0
			\end{cases}
		\end{equation*}
		Therefore, a general solution is of the form  $\inlinematrix{\frac{z}{2}\\\frac{z}{2}\\z}$
		with one degree of freedom and a geometric multiplicity of $1$. Hence,
		\begin{equation*}
			v_3 = \begin{pmatrix}
				\frac{1}{2} \\[4pt]
				\frac{1}{2} \\[4pt]
				1
			\end{pmatrix}
		\end{equation*}
		This yields the basis of eigenvectors for the domain space
		\begin{equation*}
			\mathcal{B}=\left\{
			\begin{pmatrix}
				1 \\0\\-1
			\end{pmatrix},
			\begin{pmatrix}
				0 \\1\\1
			\end{pmatrix},
			\begin{pmatrix}
				1 \\1\\2
			\end{pmatrix}
			\right\}
		\end{equation*}
		Consequently, $A$ is diagonalizable, or more precisely,
		\begin{equation*}
			A\sim\fdiag(-2,-2,4)
		\end{equation*}
		where the change of basis matrix is given by\footnote{Verify that
			$P^{-1}AP=\fdiag(\lambda_1,\dots,\lambda_n)$ and find $P^{-1}$}
		\begin{equation*}
			P = \begin{pmatrix}
				1  & 0 & 1 \\
				0  & 1 & 1 \\
				-1 & 1 & 2
			\end{pmatrix}
		\end{equation*}
	\end{flushleft}
\end{exm}

\begin{definition}\label{def-algebraic-multiplicity}
	Let $\lambda$ be an eigenvalue. The algebraic multiplicity of $\lambda$ is the
	number of times it appears as the root of the characteristic polynomial.
\end{definition}

\begin{definition}\label{def-geometric-multiplicity}
	Let $\lambda$ be an eigenvalue. The geometric multiplicity of $\lambda$ is the number
	of linearly independent eigenvectors corresponding to $\lambda$.
\end{definition}

\begin{thm}\label{thm-algebraic-geometric-multiplicity}
	This theorem comes in two parts:
	\begin{enumerate}
		\item For every eigenvalue $\lambda$, the algebraic multiplicity is always
		      greater than or equal to the geometric multiplicity of $\lambda$
		\item $A$ is diagonalizable \textit{iff} the characteristic polynomial factors
		      over the field $\mathcal{F}$ into linear factors and the algebraic multiplicity
		      is equal to the geometric multiplicity for every $\lambda$
	\end{enumerate}
\end{thm}

\begin{exm}\label{exm-not-diagonalizable}
	\hfill
	\begin{enumerate}
		\item Let $A=\inlinematrix{0&1\\-1&0}$ over $\mathbb{R}$.
		      Then the characteristic polynomial is
		      \begin{align*}
			      \det(A - \lambda I) & = \begin{vmatrix}
				      -\lambda & 1        \\
				      -1       & -\lambda
			      \end{vmatrix} \\
			                          & = \lambda^2 + 1
		      \end{align*}
		      Notice that this polynomial has no real roots, \textit{i.e.} there are no eigenvalues
		      for this matrix over $\mathbb{R}$. Hence, $A$ is not diagonalizable.
		\item Let $B=\inlinematrix{-3&1&-1\\-7&5&-1\\-6&6&-2}$.
		      Then the characteristic polynomial is
		      \begin{align*}
			      \det(A - \lambda I) & = \begin{vmatrix}
				      -3-\lambda & 1         & -1         \\
				      -7         & 5-\lambda & -1         \\
				      -6         & 6         & -2-\lambda
			      \end{vmatrix}                                                   \\
			                          & = \begin{vmatrix}
				      -2-\lambda & 1         & -1         \\
				      -2-\lambda & 5-\lambda & -1         \\
				      0          & 6         & -2-\lambda
			      \end{vmatrix}              &  & \text{$C_1 \leftarrow C_1+C_2$} \\
			                          & = -(2+\lambda) \begin{vmatrix}
				      1 & 1         & -1         \\
				      1 & 5-\lambda & -1         \\
				      0 & 6         & -2-\lambda
			      \end{vmatrix}                                      \\
			                          & = -(2+\lambda) \begin{vmatrix}
				      1 & 1         & -1         \\
				      0 & 4-\lambda & 0          \\
				      0 & 6         & -2-\lambda
			      \end{vmatrix} &  & \text{$R_2 \leftarrow R_2-R_1$} \\
			                          & = (2+\lambda)^2(4-\lambda)
		      \end{align*}
		      Note that the matrix $A$ from \pref{example}{exm-algebraic-geometrix-multiplicity}
		      had the same characteristic polynomial. In this case, $\lambda=-2$ as an algebraic
		      multiplicity of $2$.
		      \begin{align*}
			      \boxed{\lambda=-2}:\quad & B - \lambda I = \begin{pmatrix}
				      -1 & 1 & -1 \\
				      -7 & 7 & -1 \\
				      -6 & 6 & 0
			      \end{pmatrix}                                                                                             \\
			      \implies
			                               & \begin{pmatrix}
				      -1 & 1 & -1 \\
				      -7 & 7 & -1 \\
				      -6 & 6 & 0
			      \end{pmatrix}\begin{pmatrix}
				      x \\ y \\ z
			      \end{pmatrix}=\begin{pmatrix}
				      0 \\ 0 \\ 0
			      \end{pmatrix}                                                      \\
			      \xRightarrow{R_3\leftarrow R_3-R_2+R_1}
			                               & \begin{pmatrix}
				      -1 & 1 & -1 \\
				      -7 & 7 & -1 \\
				      0  & 0 & 0
			      \end{pmatrix}                                                                                                             \\
			      \implies
			                               & \frank(B + 2 I)=2                                                                                                                       \\
			      \implies
			                               & \dim(\fker{B + 2 I})=1                                                             &  & \text{\pref{theorem}{thm-rank-nullity-theorem}}
		      \end{align*}
		      Therefore, the geometric multiplicity of $\lambda=-2$ is $1$ which is less than $2$,
		      its algebraic multiplicity. Hence, by \pref{theorem}{thm-algebraic-geometric-multiplicity},
		      $B$ is not diagonalizable.
	\end{enumerate}
\end{exm}

\begin{rem}
	We just saw that $A$ from \pref{example}{exm-algebraic-geometrix-multiplicity}
	and $B$ from \pref{example}{exm-not-diagonalizable} are not similar since only
	$A$ was diagonalizable, although they have the same characteristic polynomial
	and the same eigenvalues.
\end{rem}

\begin{thm}\label{thm-similar-matrices-same-polynomial-eigenvalues}
	Similar matrices have the same characteristic polynomial and eigenvalues.
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-similar-matrices-same-polynomial-eigenvalues}.
	\begin{flushleft}
		Let $A \sim B$. Then by \pref{theorem}{thm-similar-matrices-inverse-equation}
		it follows that $A=P^{-1}BP$. By definition $P$ is invertible. Moreover,
		\begin{align*}
			\det(A - \lambda I) & = \det(P^{-1}BP - \lambda I)                                                                           \\
			                    & = \det(P^{-1}BP - \lambda P^{-1}IP)                                                                    \\
			                    & = \det(P^{-1}(B - \lambda I)P)                &  & \text{\pref{lemma}{lemma-matrix-multiplication}}    \\
			                    & = \det(P^{-1})\det(B - \lambda I)\det(P)      &  & \text{\pref{theorem}{thm-product-of-matrices}}      \\
			                    & = \frac{1}{\det(P)}\det(B - \lambda I)\det(P) &  & \text{\pref{corollary}{crl-determinant-transposed}} \\
			                    & = \det(B - \lambda I)
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{rem}
	All the shared properties mentioned so far in \pref{remark}{rem-similar-matrix-properties}
	are not sufficient for similarity. In fact they are necessary conditions.
\end{rem}

\begin{thm}
	The square matrices $A$ and $A^T$ have the same characteristic polynomial\footnote{Although
		they don't necessarily have the same eigenvectors.}.
\end{thm}

\begin{rem}\label{thm-square-matrix-properties-new-extended}
	We will use this opportunity to update the list from \pref{remark}{thm-square-matrix-properties}
	and extended in \pref{remark}{thm-square-matrix-properties-extended}. The following statements are
	all equivalent:
	\begin{enumerate}
		\item $A$ is invertible.
		\item The matrix has full rank, \textit{i.e.} $\frank(A)=n$.
		\item $A$ is row-equivalent to $I$.
		\item For any vector $b\in\mathcal{F}^n$ the system $Ax=b$ has a unique solution.
		\item The rows of $A$ are linearly independent.
		\item $\det(A)\neq0$.
		\item $A$ represents an invertible linear map $T$.
		\item $A$ represents an injective map $T$.
		\item $\fker{A}=\{0\}$.
		\item $0$ is not an eigenvalue of $A$.
		\item The free coefficient of the characteristic polynomial of $A$ is not zero.
	\end{enumerate}
\end{rem}

\begin{proof}
	Of \pref{remark}{thm-square-matrix-properties-new-extended}, statement (10) and (11).
	\begin{flushleft}
		$(10)\implies(11)$: Suppose $0$ is an eigenvalue of $A$. Then this is equivalent to
		\begin{equation*}
			\det(A - 0\cdot I) = 0 \Leftrightarrow \det(A)=0
		\end{equation*}
	\end{flushleft}
	\begin{flushleft}
		$(11)\implies(10)$: Let $p(x)=\sum_{i=0}^n a_i x^i$ be the characteristic polynomial of $A$.
		Then,
		\begin{align*}
			 & a_0 = 0                        \\
			\Leftrightarrow
			 & 0\text{ is a root of }p(x)     \\
			\Leftrightarrow
			 & 0\text{ is an eigenvalue of }A \\
			\Leftrightarrow
			 & A\text{ is not invertible}
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{thm}\label{thm-ab-ba-same-eigenvalues}
	The matrices $AB$ and $BA$ have the same eigenvalues.
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-ab-ba-same-eigenvalues}.
	\begin{flushleft}
		Let $v\neq0$ be a an eigenvector corresponding to an eigenvalue $\lambda_0$
		of $AB$ and assume that $\lambda_0\neq0$. Therefore,
		\begin{equation*}
			ABv = \lambda_0 v
		\end{equation*}
		We are looking for an eigenvector $w\neq0$ s.t. $BAw = \lambda_0 w$. Take $w=Bv$,
		then
		\begin{align*}
			BAw & = BABv          \\
			    & = B\lambda_0 v  \\
			    & = \lambda_0  Bv \\
			    & = \lambda_0 w
		\end{align*}
		If $w=Bv=0$, then
		\begin{align*}
			ABv & = A\cdot 0     \\
			    & = 0            \\
			    & = \lambda_0 v,
		\end{align*}
		so $\lambda_0=0$ which is in contradiction to the initial assumption.
	\end{flushleft}
	\begin{flushleft}
		If $\lambda_0=0$ is an eigenvalue of $AB$, then
		\begin{align*}
			 & \det(AB)=0                      \\
			\Leftrightarrow
			 & \det(BA)=0                      \\
			\Leftrightarrow
			 & \det(BA-0\cdot I)=0             \\
			\Leftrightarrow
			 & 0\text{ is an eigenvalue of }BA
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{rem}
	If $\mathcal{F}$ is algebraically closed\footnote{For example $\mathbb{C}$ is
		algebraically, but not $\mathbb{R}$}, then every matrix is similar to a matrix
	of the Jordan normal form that is composed of diagonal blocks, called Jordan blocks.
\end{rem}

\begin{thm}\label{thm-similar-to-triangular}
	If all eigenvalues of $A$ belong to $\mathcal{F}$, then $A$ is similar to a
	triangular matrix whose main diagonal elements are the eigenvalues of $A$.
\end{thm}

\begin{crl}\label{crl-similar-to-triangular}
	For such $A$ of \pref{theorem}{thm-similar-to-triangular},
	\begin{align}
		\det(A)    & =\prod_{i=1}^n\lambda_i\label{eq-similar-to-triangular:1} \\
		\ftrace(A) & =\sum_{i=1}^n\lambda_i\label{eq-similar-to-triangular:2}
	\end{align}
\end{crl}

\begin{exm}
	Let $A=
		\left(
		\begin{smallmatrix}
			3 & 1 & 1 & 1 \\
			1 & 3 & 1 & 1 \\
			1 & 1 & 3 & 1 \\
			1 & 1 & 1 & 3
		\end{smallmatrix}
		\right)$. Is this matrix diagonalizable?
	\begin{flushleft}
		\textbf{Answer}: A wise guess suggests that $2$ is an eigenvalue, because
		if we subtract $2$ from the main diagonal we get the matrix whose entries
		are all the same, and which has a determinant that is equal to zero:
		\begin{equation*}
			\det(A-2I)=0
		\end{equation*}
		Furthermore, from $\frank(A-2I)=1$ follows through the rank-nullity theorem
		that the dimension of the null space is:
		\begin{equation*}
			\dim(\fnull(A-2I))=3
		\end{equation*}
		Therefore, $\lambda=2$ has a geometric multiplicity of $3$. At this point
		in time we can narrow down the algebraic multiplicity to either $3$ or $4$
		according to \pref{theorem}{thm-algebraic-geometric-multiplicity}. So all the
		eigenvalues of $A$ are $\{2,2,2,\lambda_4\}$. From \pref{corollary}{crl-similar-to-triangular}
		we can conclude that $\ftrace(A)=12$. Hence, $\lambda_2=6$ which has a geometric
		and algebraic multiplicity of $1$. Again, by \pref{corollary}{crl-similar-to-triangular}
		it follows that $A$ is diagonalizable.
	\end{flushleft}
\end{exm}

\begin{thm}\label{thm-power-of-diagonalizable-matrix}
	Let $A$ be diagonalizable, and let $\lambda_1,\dots,\lambda_n$ be the eigenvalues.
	Moreover, let $P$ the invertible matrix with eigenvectors corresponding to these
	aforementioned eigenvalues as columns. Then,
	\begin{equation}
		A^k = P\fdiag(\lambda_1^k,\dots,\lambda_n^k)P^{-1}
	\end{equation}
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-power-of-diagonalizable-matrix}.
	\begin{flushleft}
		Let $D\defines\fdiag(\lambda_1,\dots,\lambda_n)$. Then
		\begin{align*}
			D^k & = \begin{pmatrix}
				\lambda_1^k & 0           & 0           & \cdots & 0           \\
				0           & \lambda_2^k & 0           & \cdots & 0           \\
				0           & 0           & \lambda_3^k & \cdots & 0           \\
				\vdots      & \vdots      & \vdots      & \ddots & \vdots      \\
				0           & 0           & 0           & \cdots & \lambda_n^k
			\end{pmatrix}                                                                                                          \\
			    & = (P^{-1}DP)^k                                                       &  & \text{\pref{theorem}{thm-similar-matrices-inverse-equation}} \\
			    & = \underbrace{(P^{-1}DP)(P^{-1}DP)\cdots(P^{-1}DP)}_{k\text{-times}}                                                                   \\
			    & = P^{-1}A^kP
		\end{align*}
		Hence,
		\begin{align*}
			D^k                   & = P^{-1}A^kP \\
			\implies P D^k        & = A^kP       \\
			\implies P D^k P^{-1} & = A^k
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{thm}\label{thm-cayley-hamilton-theorem}
	The Cayley Hamilton theorem states that a general invertible matrix $A\in\mathcal{M}_n(\mathcal{F})$
	with a characteristic polynomial $p(\lambda)$ can be written as a $n-1$'th
	polynomial expression
	\begin{equation}
		p(A)=A^n+a_{n-1}A^{n-1}+\cdots+a_1A+(-1)^n\det(A)I_n=0_n
	\end{equation}
	In other words, every matrix is a root of its characteristic polynomial.
\end{thm}

\begin{exm}
	Let $A=\inlinematrix{1&2\\3&2}$. Then
	\begin{align*}
		p(\lambda) & =\begin{vmatrix}
			1-\lambda & 2         \\
			3         & 2-\lambda
		\end{vmatrix} \\
		           & =(1-\lambda)(2-\lambda)      \\
		           & =\lambda^2-3\lambda-4
	\end{align*}
	Using \pref{theorem}{thm-cayley-hamilton-theorem} we can see that
	\begin{align*}
		p(A) & =\begin{pmatrix}
			1 & 2 \\3&2
		\end{pmatrix}\begin{pmatrix}
			1 & 2 \\3&2
		\end{pmatrix}-3\begin{pmatrix}
			1 & 2 \\3&2
		\end{pmatrix}-4\begin{pmatrix}
			1 & 0 \\0&1
		\end{pmatrix} \\
		     & =\begin{pmatrix}
			7 & 6 \\9&10
		\end{pmatrix}-
		\begin{pmatrix}
			3 & 6 \\9&6
		\end{pmatrix}-
		\begin{pmatrix}
			4 & 0 \\0&4
		\end{pmatrix}                                                                                              \\
		     & =\begin{pmatrix}
			0 & 0 \\0&0
		\end{pmatrix}
	\end{align*}
\end{exm}
