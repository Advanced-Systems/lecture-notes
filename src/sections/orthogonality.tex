\subsection{Orthogonality}\label{subsec-orthogonality}

\begin{definition}\label{def-unit-vector}
	The unit vector $\hat{v}$ is a normalized non-zero vector $v\neq0$ of the form
	\begin{equation}\label{eq-unit-vector}
		\hat{v}=\frac{v}{\norm{v}}
	\end{equation}
\end{definition}

\begin{rem}\label{rem-gram-schmidt-algorithm}
	A more sophisticated process called Gram-Schmidt algorithm enables one to take
	any linearly independent set and produce an orthonormal set with the same span.
\end{rem}

\begin{rem}
	Note that the canonical standard basis $\mathcal{E}=\{e_1,e_2,e_3\}$ of
	$\mathbb{R}^3$ satisfies the following two properties for all $i\in\{1,2,3\}$:
	\begin{enumerate}
		\item $\norm{e_i}=1$
		\item $\left<e_i,e_j\right>=0$ for all $i \neq j$
	\end{enumerate}
	Each of these standard basis elements are therefore called unit vectors.
	From \pref{equation}{eq-cos-of-vectors} and property 2 also follows that two
	different unit vectors of $\mathcal{E}$ are pairwise perpendicular.
\end{rem}

\begin{definition}\label{def-orthogonal-vectors}
	Two vectors $u,w\in\mathcal{V}$ are called perpendicular (or orthogonal),
	\textit{i.e.}
	\begin{equation}\label{eq-orthogonal-vectors}
		u \perp v \Leftrightarrow \left<u,v\right>=0
	\end{equation}
\end{definition}

\begin{definition}\label{def-orthonormal-basis}
	A set $\mathcal{B}=\{v_1,\dots,v_n\}$ in an inner product space $\mathcal{V}$
	is called orthogonal if $\left<v_i,v_j\right>=0$ for all $i\neq j$. In addition
	to this definition, if $\norm{v_i}=1$ for all $i\in\{1,\dots,n\}$, then
	$\mathcal{B}$ is called orthonormal. Furthermore, if $\mathcal{B}$ is a basis
	for $\mathcal{V}$, then it is called an orthogonal (or orthonormal) basis.
\end{definition}

\begin{thm}\label{thm-orthonormal-basis-coefficients}
	If $\mathcal{B}=\{v_1,\dots,v_n\}$ is an orthonormal basis of $\mathcal{V}$,
	then for every $v\in\mathcal{V}$,
	\begin{equation}
		v=\sum_{i=1}^n \left<v,v_i\right>v_i
	\end{equation}
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-orthonormal-basis-coefficients}.
	\begin{flushleft}
		Let $v=\lambda_1v_1+\cdots+\lambda_nv_n$. Then
		\begin{align*}
			\left<v,v_i\right> & = \left<\lambda_1v_1+\cdots+\lambda_nv_n,v_i\right>                                                                    \\
			                   & = \left<\lambda_1v_1,v_i\right>+\cdots+\left<\lambda_nv_n,v_i\right> &  & \text{\pref{definition}{def-inner-product}}  \\
			                   & = \lambda_1\left<v_1,v_i\right>+\cdots+\lambda_n\left<v_n,v_i\right> &  & \text{\pref{definition}{def-inner-product}}  \\
			                   & = \lambda_i\left<v_i,v_i\right>                                      &  & \text{\pref{theorem}{def-orthonormal-basis}} \\
			                   & = \lambda_i\norm{v_i}^2                                              &  & \text{\pref{theorem}{def-orthonormal-basis}} \\
			                   & = \lambda_i
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{thm}\label{thm-orthonormal-basis-norm}
	If $\mathcal{B}=\{v_1,\dots,v_n\}$ is an orthonormal basis of $\mathcal{V}$,
	then
	\begin{equation}
		\norm{v}=\sqrt{\sum_{i=1}^n \abs{\left<v,v_i\right>}^2}
	\end{equation}
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-orthonormal-basis-norm}.
	\begin{flushleft}
		By definition we know that $\norm{v}=\sqrt{\left<v,v\right>}$. So,
		\begin{align*}
			\left<v,v\right> & = \left<\sum_{i=1}^n \left<v,v_i\right>v_i, \sum_{i=1}^n \left<v,v_i\right>v_i\right> &  & \text{\pref{theorem}{thm-orthonormal-basis-coefficients}} \\
			                 & = \sum_{i=1}^n \left<v,v_i\right>\overline{\left<v,v_i\right>} \left<v_i,v_i\right>   &  & \text{\pref{theorem}{def-orthonormal-basis}}              \\
			                 & = \sum_{i=1}^n \abs{\left<v,v_i\right>}^2
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{thm}\label{thm-orthogonal-diagonalization}
	Let $A\in\mathcal{M}_n(\mathbb{R})$. Then $A$ admits orthogonal diagonalization,
	\textit{i.e.}
	\begin{equation}
		D = P^{-1}AP
	\end{equation}
	where $D$ is diagonalization, and $P$ is an orthogonal matrix, meaning that
	the columns of $P$ (which are the eigenvectors of $A$) are orthonormal
	\footnote{This is equivalent to saying $P^{-1}=P^T$ which is the traditional
		definition of an orthogonal matrix.}.
\end{thm}
