\subsection{Similarity of Matrices}\label{subsec-similarity-of-matrices}

\begin{exm}\label{exm-similar-matrices}
	Let $T:\mathbb{R}^3\to\mathbb{R}^3$ be a linear operator defined by
	\begin{equation*}
		T\begin{pmatrix}
			a \\ b \\ c
		\end{pmatrix}=\begin{pmatrix}
			-a \\ b \\ 0
		\end{pmatrix}
	\end{equation*}
	with $\mathcal{E}=\{e_1,e_2,e_3\}$ as the canonical basis for the domain of
	$T$, and let $\mathcal{F}$ be a basis for the range of $T$ given by
	\begin{equation*}
		\mathcal{F}=\left\{
		\begin{pmatrix}
			1 \\ 1 \\ 0
		\end{pmatrix},
		\begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix},
		\begin{pmatrix}
			0 \\ 1 \\ 1
		\end{pmatrix}
		\right\}=\{f_1,f_2,f_3\}
	\end{equation*}
	Then the matrix representation of $T$ with respect to $\mathcal{E}$ is
	\begin{align*}
		T(e_1)          & =T\begin{pmatrix}1\\0\\0\end{pmatrix}=\begin{pmatrix}-1\\0\\0\end{pmatrix}=-e_1 \\
		T(e_2)          & =T\begin{pmatrix}0\\1\\0\end{pmatrix}=\begin{pmatrix}0\\1\\0\end{pmatrix}=e_2  \\
		T(e_3)          & =T\begin{pmatrix}0\\0\\1\end{pmatrix}=\begin{pmatrix}0\\0\\0\end{pmatrix}=0  \\
		\implies
		[T]_\mathcal{E} & =\begin{pmatrix}
			-1 & 0 & 0 \\
			0  & 1 & 0 \\
			0  & 0 & 0
		\end{pmatrix}
	\end{align*}
	We repeat this procedure one more time with respect to the basis $\mathcal{F}$.
	Recall from \pref{example}{exm-vector-basis:2} that we have already found the
	formula for representing a vector with respect to basis $\mathcal{F}$ by
	\begin{equation*}
		\begin{pmatrix}
			a \\b\\c
		\end{pmatrix}=
		\frac{a-c+b}{2}\begin{pmatrix}
			1 \\1\\0
		\end{pmatrix}+
		\frac{c-b+a}{2}\begin{pmatrix}
			1 \\0\\1
		\end{pmatrix}+
		\frac{b-a+c}{2}\begin{pmatrix}
			0 \\1\\1
		\end{pmatrix}
	\end{equation*}
	We can use this to write
	\begin{align*}
		T(f_1)          & =T\begin{pmatrix}1\\1\\0\end{pmatrix}=\begin{pmatrix}-1\\1\\0\end{pmatrix}=-f_1+f_3                                      \\
		T(f_2)          & =T\begin{pmatrix}1\\0\\1\end{pmatrix}=\begin{pmatrix}-1\\0\\0\end{pmatrix}=-\frac{1}{2}f_1-\frac{1}{2}f_2+\frac{1}{2}f_3 \\
		T(f_3)          & =T\begin{pmatrix}0\\1\\1\end{pmatrix}=\begin{pmatrix}0\\1\\0\end{pmatrix}=\frac{1}{2}f_1-\frac{1}{2}f_2+\frac{1}{2}f_3  \\
		\implies
		[T]_\mathcal{F} & =\begin{pmatrix}
			0  & -\frac{1}{2} & \frac{1}{2}  \\[4pt]
			-1 & -\frac{1}{2} & -\frac{1}{2} \\[4pt]
			1  & \frac{1}{2}  & \frac{1}{2}
		\end{pmatrix}
	\end{align*}
	Well, how are $[T]_\mathcal{E}$ and $[T]_\mathcal{F}$ related with each other?
	The answer to this question can be found in the next definition!
\end{exm}

\begin{definition}\label{def-similar-matrices}
	Two matrices $A$ and $B$ are called similar if they represent the same linear
	map $T$ with respect to different bases and is denoted by $A \sim B$
	\footnote{This is an equivalence relation, \textit{cf.} \pref{definition}{def-equivalence-relation}}.
\end{definition}

\begin{rem}\label{rem-similar-matrix-properties}
	We will see in a moment that similar matrices share many properties
	\footnote{Note that this is not a complete list} with each other:
	\begin{itemize}
		\item Similar matrices have the same rank
		\item Similar matrices have the same determinant
		\item Similar matrices have the same trace
		\item Similar matrices have the same eigenvalues
		\item Similar matrices have the same characteristic polynomial
	\end{itemize}
\end{rem}

\begin{thm}\label{thm-similar-matrices-inverse-equation}
	Let $A$ and $B$ be two matrices. Then, $A \sim B$ \textit{iff} there exists
	an invertible matrix $P$ s.t.
	\begin{equation}
		B=P^{-1}AP
	\end{equation}
\end{thm}

\begin{rem}\label{rem-change-of-basis-matrix}
	Referring to \pref{theorem}{thm-similar-matrices-inverse-equation}, such a
	matrix $P$ can be found explicitly by
	\begin{equation}\label{eq-change-of-basis-matrix}
		P=[I]_\mathcal{F}^\mathcal{E}
	\end{equation}
	where $I$ is the identity map from $\mathcal{V}$ to $\mathcal{V}$ with respect to
	the bases $\mathcal{E}$ and $\mathcal{F}$. Interestingly enough, $P$ also has a
	special name: it is known as the change of basis matrix.
\end{rem}

\begin{thm}\label{thm-change-of-basis-matrix-properties}
	Let $P$ be the change of basis matrix with respect to $\mathcal{E}$ and
	$\mathcal{F}$, \textit{cf.} \pref{remark}{rem-change-of-basis-matrix}. Then,
	\begin{enumerate}
		\item $P[v]_\mathcal{F}=[v]_\mathcal{E}$
		\item $P^{-1}[v]_\mathcal{E}=[v]_\mathcal{F}$
		\item $P^{-1}[T]_\mathcal{E}P=[T]_\mathcal{F}$
	\end{enumerate}
\end{thm}

\begin{exm}\label{exm-change-of-basis-matrix}
	Continuing with \pref{example}{exm-similar-matrices}, let's try to find the
	change of basis matrix with respect to $\mathcal{E}$ and $\mathcal{F}$, \textit{i.e.}
	$P=[I]_\mathcal{F}^\mathcal{E}$:
	\begin{align*}
		I(f_1) & =I\begin{pmatrix}1\\1\\0\end{pmatrix}=e_1+e_2 \\
		I(f_2) & =I\begin{pmatrix}1\\0\\1\end{pmatrix}=e_1+e_3 \\
		I(f_3) & =I\begin{pmatrix}0\\1\\1\end{pmatrix}=e_2+e_3 \\
		\implies
		P      & =\begin{pmatrix}
			1 & 1 & 0 \\
			1 & 0 & 1 \\
			0 & 1 & 1
		\end{pmatrix}
	\end{align*}
	To find $P^{-1}$ we can either calculate $[I]_\mathcal{E}^\mathcal{F}$ or
	use the method of row reduction to transform $(P|I)$ to $(I|P^{-1})$. It's
	arguably easier to use the former method. Either way the inverse of the change
	of basis matrix can be found as
	\begin{align*}
		P^{-1}=\begin{pmatrix}
			\frac{1}{2}  & \frac{1}{2}  & -\frac{1}{2} \\[4pt]
			\frac{1}{2}  & -\frac{1}{2} & \frac{1}{2}  \\[4pt]
			-\frac{1}{2} & \frac{1}{2}  & \frac{1}{2}
		\end{pmatrix}
	\end{align*}
\end{exm}

\begin{exm}
	Based on the results of \pref{example}{exm-change-of-basis-matrix} we do some
	more examples for \pref{theorem}{thm-change-of-basis-matrix-properties}.
	Let $v=\inlinematrix{1\\2\\3}$. Recall that
	\begin{align*}
		[v]_\mathcal{E} & =\begin{pmatrix}
			1 \\2\\3
		\end{pmatrix} \\
		[v]_\mathcal{F} & =\begin{pmatrix}
			0 \\1\\2
		\end{pmatrix}
	\end{align*}
	As for the third claim of this theorem, recall from \pref{example}{exm-similar-matrices} that
	\begin{align*}
		[T]_\mathcal{E}[v]_\mathcal{E} & =[T(v)]_\mathcal{E}                                   \\
		                               & =\begin{pmatrix}
			-1 & 0 & 0 \\
			0  & 1 & 0 \\
			0  & 0 & 0
		\end{pmatrix}\begin{pmatrix}
			a \\b\\c
		\end{pmatrix} \\
		                               & =\begin{pmatrix}
			-a \\b\\0
		\end{pmatrix}
	\end{align*}
	In the same fashion we have
	\begin{align*}
		[T]_\mathcal{F}[v]_\mathcal{F} & =[T(v)]_\mathcal{F}                                   \\
		                               & =\begin{pmatrix}
			0  & -\frac{1}{2} & \frac{1}{2}  \\[4pt]
			-1 & -\frac{1}{2} & -\frac{1}{2} \\[4pt]
			1  & \frac{1}{2}  & \frac{1}{2}
		\end{pmatrix}\begin{pmatrix}
			\frac{a-c+b}{2} \\[4pt]
			\frac{a-b+c}{2} \\[4pt]
			\frac{b-a+c}{2}
		\end{pmatrix} \\
		                               & =\begin{pmatrix}
			\frac{-a+b}{2} \\[4pt]
			\frac{-a-b}{2} \\[4pt]
			\frac{a+b}{2}
		\end{pmatrix}
	\end{align*}
	\begin{enumerate}
		\item First claim of this theorem on example of vector $v$:
		      \begin{align*}
			      P[v]_\mathcal{F} & =\begin{pmatrix}
				      1 & 1 & 0 \\
				      1 & 0 & 1 \\
				      0 & 1 & 1
			      \end{pmatrix}\begin{pmatrix}
				      0 \\1\\2
			      \end{pmatrix} \\
			                       & =\begin{pmatrix}
				      1 \\2\\3
			      \end{pmatrix}                           \\
			                       & =[v]_\mathcal{E}
		      \end{align*}
		\item Second claim of this theorem on example of vector $v$:
		      \begin{align*}
			      P^{-1}[v]_\mathcal{E} & =\begin{pmatrix}
				      \frac{1}{2}  & \frac{1}{2}  & -\frac{1}{2} \\[4pt]
				      \frac{1}{2}  & -\frac{1}{2} & \frac{1}{2}  \\[4pt]
				      -\frac{1}{2} & \frac{1}{2}  & \frac{1}{2}
			      \end{pmatrix}\begin{pmatrix}
				      1 \\2\\3
			      \end{pmatrix} \\
			                            & =\begin{pmatrix}
				      0 \\1\\2
			      \end{pmatrix}                           \\
			                            & =[v]_\mathcal{F}
		      \end{align*}
		\item Third claim of this theorem on example of vector $v$:
		      \begin{align*}
			      P^{-1}[T]_\mathcal{E}P & =\begin{pmatrix}
				      \frac{1}{2}  & \frac{1}{2}  & -\frac{1}{2} \\[4pt]
				      \frac{1}{2}  & -\frac{1}{2} & \frac{1}{2}  \\[4pt]
				      -\frac{1}{2} & \frac{1}{2}  & \frac{1}{2}
			      \end{pmatrix}\begin{pmatrix}
				      -1 & 0 & 0 \\
				      0  & 1 & 0 \\
				      0  & 0 & 0
			      \end{pmatrix}\begin{pmatrix}
				      1 & 1 & 0 \\
				      1 & 0 & 1 \\
				      0 & 1 & 1
			      \end{pmatrix} \\
			                             & =\begin{pmatrix}
				      0  & -\frac{1}{2} & \frac{1}{2}  \\[4pt]
				      -1 & -\frac{1}{2} & -\frac{1}{2} \\[4pt]
				      1  & \frac{1}{2}  & \frac{1}{2}
			      \end{pmatrix}                                                     \\
			                             & =[T]_\mathcal{F}
		      \end{align*}
	\end{enumerate}
\end{exm}

\begin{rem}
	In summary, this subsection was motivated by the following two questions:
	\begin{itemize}
		\item Given a linear transformation $T$, how can we find the \enquote{best} matrix
		      representation?
		\item Given two matrices $A$ and $B$, how do we know if they are similar?
	\end{itemize}
\end{rem}

\begin{definition}\label{def-matrix-trace}
	The sum of the main diagonal elements of a square matrix is called the trace
	of $A$ and is denoted by
	\begin{equation}
		\ftrace(A)\defines\sum_{i=1}^n a_{ii}
	\end{equation}
\end{definition}

\begin{rem}\label{rem-trace-of-transposed}
	A matrix and its transpose have the same trace, \textit{i.e.}
	\begin{equation}
		\ftrace(A)=\ftrace(A^T)
	\end{equation}
	which follows immediately from the fact that transposing a matrix has no effect
	on its main diagonal elements.
\end{rem}

\begin{exm}
	The trace of the matrix $A=\inlinematrix{\pi&3&\sqrt{2}\\-1&e&\tfrac{1}{2}\\8&\pi&0}$ is
	\begin{equation*}
		\ftrace(A)=\pi + e + 0 = \pi + e
	\end{equation*}
\end{exm}

\begin{thm}\label{thm-trace-product-commutative}
	For two square matrices $A$ and $B$, the product of their traces is commutative,
	\textit{i.e.}
	\begin{equation}
		\ftrace(AB)=\ftrace(BA)
	\end{equation}
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-trace-product-commutative}.
	\begin{flushleft}
		In general, for $A,B\in\mathcal{M}_n{\mathcal{F}}$ we can write the product
		of the diagonal elements of two matrices as
		\begin{align*}
			(AB)_{ij}          & = \sum_{k=1}^n a_{ik}b_{kj} \\
			\implies (AB)_{ii} & = \sum_{k=1}^n a_{ik}b_{ki}
		\end{align*}
		Therefore, we can write the product of this trace as
		\begin{align*}
			\ftrace(AB) & = \sum_{i=1}^n (AB)_{ii}                 \\
			            & = \sum_{i=1}^n \sum_{k=1}^n a_{ik}b_{ki} \\
			            & = \sum_{i=1}^n \sum_{k=1}^n b_{ik}a_{ki} \\
			            & = \sum_{i=1}^n (BA)_{ii}                 \\
			            & = \ftrace(BA)
		\end{align*}
	\end{flushleft}
\end{proof}

\begin{thm}\label{thm-similar-matrix-properties:1}
	Suppose that for two matrices $A$ and $B$, $A \sim B$. Then\footnote{See also
		\pref{remark}{rem-similar-matrix-properties}}:
	\begin{enumerate}
		\item $\det(A)=\det(B)$
		\item $\ftrace(A)=\ftrace(B)$
		\item $\frank(A)=\frank(B)$
	\end{enumerate}
\end{thm}

\begin{proof}
	Of \pref{theorem}{thm-similar-matrix-properties:1}.
	\begin{flushleft}
		\textbf{Statement 1}: Since $A \sim B$ we can write this as\footnote{Note
			that by definition, $P$ is an invertible matrix, therefore the determinant
			is not zero by \pref{theorem}{thm-square-matrix-properties-extended}}
		\begin{align*}
			\det(B) & = \det(P^{-1}AP)                  &  & \text{\pref{theorem}{thm-similar-matrices-inverse-equation}} \\
			        & = \det(P^{-1})\det(A)\det(P)      &  & \text{\pref{theorem}{thm-product-of-matrices}}               \\
			        & = \frac{1}{\det(P)}\det(A)\det(P) &  & \text{\pref{corollary}{crl-determinant-transposed}}          \\
			        & = \det(A)
		\end{align*}
	\end{flushleft}
	\begin{flushleft}
		\textbf{Statement 2}: Since $A \sim B$ we can write this as
		\begin{align*}
			\ftrace(B) & = \ftrace(P^{-1}AP) &  & \text{\pref{theorem}{thm-similar-matrices-inverse-equation}} \\
			           & = \ftrace(P^{-1}PA) &  & \text{\pref{theorem}{thm-trace-product-commutative}}         \\
			           & = \ftrace(A)        &  & \text{\pref{definition}{def-invertible-matrix}}
		\end{align*}
	\end{flushleft}
\end{proof}
